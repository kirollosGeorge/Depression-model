{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the real reason why you're sad? you're attache...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>my biggest problem is overthinking everything</td>\n",
       "      <td>depressingmsgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the worst sadness is the sadness you've taught...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i cannot make you understand. i cannot make an...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i don't think anyone really understands how ti...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              tweet        username\n",
       "0       1  the real reason why you're sad? you're attache...  depressingmsgs\n",
       "1       1      my biggest problem is overthinking everything  depressingmsgs\n",
       "2       1  the worst sadness is the sadness you've taught...  depressingmsgs\n",
       "3       1  i cannot make you understand. i cannot make an...  depressingmsgs\n",
       "4       1  i don't think anyone really understands how ti...  depressingmsgs"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Acc 84/TwitterDataset.csv\",encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = [\"target\", \"TweetText\", \"user\"]\n",
    "data.columns = DATASET_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8135.000000</td>\n",
       "      <td>8135</td>\n",
       "      <td>8135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7767</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>do you ever feel ok but your sad at the same t...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.422864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.494045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target                                          TweetText  \\\n",
       "count   8135.000000                                               8135   \n",
       "unique          NaN                                               7767   \n",
       "top             NaN  do you ever feel ok but your sad at the same t...   \n",
       "freq            NaN                                                 10   \n",
       "mean       0.422864                                                NaN   \n",
       "std        0.494045                                                NaN   \n",
       "min        0.000000                                                NaN   \n",
       "25%        0.000000                                                NaN   \n",
       "50%        0.000000                                                NaN   \n",
       "75%        1.000000                                                NaN   \n",
       "max        1.000000                                                NaN   \n",
       "\n",
       "                  user  \n",
       "count             8135  \n",
       "unique              12  \n",
       "top     depressingmsgs  \n",
       "freq               998  \n",
       "mean               NaN  \n",
       "std                NaN  \n",
       "min                NaN  \n",
       "25%                NaN  \n",
       "50%                NaN  \n",
       "75%                NaN  \n",
       "max                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target        int64\n",
       "TweetText    object\n",
       "user         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8135 entries, 0 to 8134\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   target     8135 non-null   int64 \n",
      " 1   TweetText  8135 non-null   object\n",
      " 2   user       8135 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 190.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kerillos\\AppData\\Local\\Temp\\ipykernel_12924\\3300746211.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(r\"http\\S+\", \"\")\n",
      "C:\\Users\\Kerillos\\AppData\\Local\\Temp\\ipykernel_12924\\3300746211.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>user</th>\n",
       "      <th>Clean_TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the real reason why you're sad? you're attache...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>real reason sad attached people distant paying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>my biggest problem is overthinking everything</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>biggest problem overthinking everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the worst sadness is the sadness you've taught...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>worst sadness sadness taught hide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i cannot make you understand. i cannot make an...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>cannot make understand cannot make anyone unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i don't think anyone really understands how ti...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>think anyone really understands tiring act oka...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                          TweetText            user  \\\n",
       "0       1  the real reason why you're sad? you're attache...  depressingmsgs   \n",
       "1       1      my biggest problem is overthinking everything  depressingmsgs   \n",
       "2       1  the worst sadness is the sadness you've taught...  depressingmsgs   \n",
       "3       1  i cannot make you understand. i cannot make an...  depressingmsgs   \n",
       "4       1  i don't think anyone really understands how ti...  depressingmsgs   \n",
       "\n",
       "                                     Clean_TweetText  \n",
       "0  real reason sad attached people distant paying...  \n",
       "1            biggest problem overthinking everything  \n",
       "2                  worst sadness sadness taught hide  \n",
       "3  cannot make understand cannot make anyone unde...  \n",
       "4  think anyone really understands tiring act oka...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Twitter Handles (@user)\n",
    "data['Clean_TweetText'] = data['TweetText'].str.replace(\"@\", \"\") \n",
    "# Removing links\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(r\"http\\S+\", \"\") \n",
    "# Removing Punctuations, Numbers, and Special Characters\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "# Remove stop words\n",
    "import nltk\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return clean_text\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda text : remove_stopwords(text.lower()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>user</th>\n",
       "      <th>Clean_TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the real reason why you're sad? you're attache...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>[real, reason, sad, attached, people, distant,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>my biggest problem is overthinking everything</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>[biggest, problem, overthinking, everything]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the worst sadness is the sadness you've taught...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>[worst, sadness, sadness, taught, hide]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i cannot make you understand. i cannot make an...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>[can, not, make, understand, can, not, make, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i don't think anyone really understands how ti...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>[think, anyone, really, understands, tiring, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                          TweetText            user  \\\n",
       "0       1  the real reason why you're sad? you're attache...  depressingmsgs   \n",
       "1       1      my biggest problem is overthinking everything  depressingmsgs   \n",
       "2       1  the worst sadness is the sadness you've taught...  depressingmsgs   \n",
       "3       1  i cannot make you understand. i cannot make an...  depressingmsgs   \n",
       "4       1  i don't think anyone really understands how ti...  depressingmsgs   \n",
       "\n",
       "                                     Clean_TweetText  \n",
       "0  [real, reason, sad, attached, people, distant,...  \n",
       "1       [biggest, problem, overthinking, everything]  \n",
       "2            [worst, sadness, sadness, taught, hide]  \n",
       "3  [can, not, make, understand, can, not, make, a...  \n",
       "4  [think, anyone, really, understands, tiring, a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Tokenization and Normalization\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: nltk.word_tokenize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>user</th>\n",
       "      <th>Clean_TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the real reason why you're sad? you're attache...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>real reason attached people distant paying att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>my biggest problem is overthinking everything</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>biggest problem overthinking everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the worst sadness is the sadness you've taught...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>worst sadness sadness taught hide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i cannot make you understand. i cannot make an...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>make understand make anyone understand happeni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i don't think anyone really understands how ti...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>think anyone really understands tiring okay al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                          TweetText            user  \\\n",
       "0       1  the real reason why you're sad? you're attache...  depressingmsgs   \n",
       "1       1      my biggest problem is overthinking everything  depressingmsgs   \n",
       "2       1  the worst sadness is the sadness you've taught...  depressingmsgs   \n",
       "3       1  i cannot make you understand. i cannot make an...  depressingmsgs   \n",
       "4       1  i don't think anyone really understands how ti...  depressingmsgs   \n",
       "\n",
       "                                     Clean_TweetText  \n",
       "0  real reason attached people distant paying att...  \n",
       "1            biggest problem overthinking everything  \n",
       "2                  worst sadness sadness taught hide  \n",
       "3  make understand make anyone understand happeni...  \n",
       "4  think anyone really understands tiring okay al...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let’s stitch these tokens back together\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x]))\n",
    "# Removing small words\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(data['Clean_TweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1000)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kerillos\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'accept',\n",
       " 'account',\n",
       " 'achieve',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actsoffaith',\n",
       " 'actually',\n",
       " 'admit',\n",
       " 'advice',\n",
       " 'affirmation',\n",
       " 'afraid',\n",
       " 'aired',\n",
       " 'alarm',\n",
       " 'alive',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'already',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'america',\n",
       " 'american',\n",
       " 'android',\n",
       " 'anger',\n",
       " 'angry',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'anti',\n",
       " 'anxiety',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appreciate',\n",
       " 'april',\n",
       " 'around',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'attention',\n",
       " 'audio',\n",
       " 'austin',\n",
       " 'authorities',\n",
       " 'available',\n",
       " 'avoid',\n",
       " 'awake',\n",
       " 'awakenings',\n",
       " 'awakeningsapp',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'badly',\n",
       " 'based',\n",
       " 'basically',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'become',\n",
       " 'becoming',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'beloveds',\n",
       " 'best',\n",
       " 'better',\n",
       " 'beyond',\n",
       " 'biden',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'billion',\n",
       " 'birthday',\n",
       " 'bitch',\n",
       " 'bitcoin',\n",
       " 'black',\n",
       " 'body',\n",
       " 'book',\n",
       " 'born',\n",
       " 'bother',\n",
       " 'bought',\n",
       " 'boundaries',\n",
       " 'boyfriend',\n",
       " 'brain',\n",
       " 'break',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breathing',\n",
       " 'bring',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bullshit',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'busy',\n",
       " 'buying',\n",
       " 'cake',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'care',\n",
       " 'career',\n",
       " 'cares',\n",
       " 'case',\n",
       " 'catch',\n",
       " 'cause',\n",
       " 'celebrate',\n",
       " 'challenge',\n",
       " 'challenges',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'chapters',\n",
       " 'charge',\n",
       " 'check',\n",
       " 'cheer',\n",
       " 'chess',\n",
       " 'child',\n",
       " 'children',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choose',\n",
       " 'christmas',\n",
       " 'city',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'club',\n",
       " 'clubhouse',\n",
       " 'coaching',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'college',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'committed',\n",
       " 'community',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'competition',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'conan',\n",
       " 'conanghana',\n",
       " 'conangreenland',\n",
       " 'confidence',\n",
       " 'content',\n",
       " 'continue',\n",
       " 'control',\n",
       " 'conversation',\n",
       " 'conversations',\n",
       " 'cool',\n",
       " 'coronavirus',\n",
       " 'could',\n",
       " 'country',\n",
       " 'courage',\n",
       " 'course',\n",
       " 'covid',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'created',\n",
       " 'crisis',\n",
       " 'crying',\n",
       " 'crypto',\n",
       " 'culture',\n",
       " 'customers',\n",
       " 'cute',\n",
       " 'cuts',\n",
       " 'daily',\n",
       " 'dailyguidance',\n",
       " 'dailyprinciples',\n",
       " 'damn',\n",
       " 'dance',\n",
       " 'dark',\n",
       " 'date',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'deep',\n",
       " 'demons',\n",
       " 'depressed',\n",
       " 'depression',\n",
       " 'deserve',\n",
       " 'desire',\n",
       " 'destroy',\n",
       " 'determined',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'difficult',\n",
       " 'direction',\n",
       " 'disappear',\n",
       " 'disappointed',\n",
       " 'disappointment',\n",
       " 'discuss',\n",
       " 'divine',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'doubt',\n",
       " 'download',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'dressed',\n",
       " 'drink',\n",
       " 'drowning',\n",
       " 'dumb',\n",
       " 'dying',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easier',\n",
       " 'easy',\n",
       " 'eating',\n",
       " 'either',\n",
       " 'election',\n",
       " 'else',\n",
       " 'email',\n",
       " 'emotional',\n",
       " 'emotionally',\n",
       " 'emotions',\n",
       " 'employees',\n",
       " 'empty',\n",
       " 'energy',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'episode',\n",
       " 'episodes',\n",
       " 'escape',\n",
       " 'esports',\n",
       " 'esteem',\n",
       " 'even',\n",
       " 'event',\n",
       " 'events',\n",
       " 'eventually',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everytime',\n",
       " 'exactly',\n",
       " 'excited',\n",
       " 'exist',\n",
       " 'expect',\n",
       " 'experience',\n",
       " 'experiences',\n",
       " 'explain',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fail',\n",
       " 'failure',\n",
       " 'faith',\n",
       " 'fake',\n",
       " 'fall',\n",
       " 'falling',\n",
       " 'family',\n",
       " 'fans',\n",
       " 'fast',\n",
       " 'fault',\n",
       " 'favorite',\n",
       " 'fear',\n",
       " 'fearnot',\n",
       " 'feature',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feelings',\n",
       " 'feels',\n",
       " 'felt',\n",
       " 'fight',\n",
       " 'fighting',\n",
       " 'figure',\n",
       " 'filled',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finished',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'five',\n",
       " 'fixmylife',\n",
       " 'focus',\n",
       " 'follow',\n",
       " 'followers',\n",
       " 'food',\n",
       " 'force',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgive',\n",
       " 'forgiveness',\n",
       " 'forgot',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'founder',\n",
       " 'four',\n",
       " 'free',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'front',\n",
       " 'fuck',\n",
       " 'fucked',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'funny',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gave',\n",
       " 'genius',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'ghana',\n",
       " 'girl',\n",
       " 'girlfriend',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'goal',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'grateful',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatest',\n",
       " 'greatness',\n",
       " 'greenland',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growing',\n",
       " 'growth',\n",
       " 'guess',\n",
       " 'guest',\n",
       " 'guide',\n",
       " 'guided',\n",
       " 'guys',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'halloween',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'hang',\n",
       " 'hanging',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'happens',\n",
       " 'happiest',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'harder',\n",
       " 'hardest',\n",
       " 'hate',\n",
       " 'hates',\n",
       " 'head',\n",
       " 'heal',\n",
       " 'healing',\n",
       " 'health',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'hearing',\n",
       " 'heart',\n",
       " 'hearts',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'helps',\n",
       " 'hide',\n",
       " 'high',\n",
       " 'hilarious',\n",
       " 'hitting',\n",
       " 'hold',\n",
       " 'holding',\n",
       " 'home',\n",
       " 'honest',\n",
       " 'honestly',\n",
       " 'honor',\n",
       " 'hope',\n",
       " 'hospital',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'however',\n",
       " 'human',\n",
       " 'hurt',\n",
       " 'hurting',\n",
       " 'hurts',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'ignore',\n",
       " 'ignored',\n",
       " 'imagine',\n",
       " 'immortals',\n",
       " 'impact',\n",
       " 'important',\n",
       " 'impossible',\n",
       " 'improved',\n",
       " 'infinite',\n",
       " 'innovation',\n",
       " 'insecure',\n",
       " 'inside',\n",
       " 'inspire',\n",
       " 'inspired',\n",
       " 'inspiring',\n",
       " 'instagram',\n",
       " 'instead',\n",
       " 'interest',\n",
       " 'interesting',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'invest',\n",
       " 'issues',\n",
       " 'iyanla',\n",
       " 'iyanlafixmylife',\n",
       " 'iyanlaguidance',\n",
       " 'iyanlameditations',\n",
       " 'iyanlavanzant',\n",
       " 'jealous',\n",
       " 'jesus',\n",
       " 'john',\n",
       " 'join',\n",
       " 'joined',\n",
       " 'journey',\n",
       " 'judge',\n",
       " 'judging',\n",
       " 'july',\n",
       " 'june',\n",
       " 'justaminx',\n",
       " 'keep',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowing',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'lack',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'lead',\n",
       " 'leader',\n",
       " 'leaders',\n",
       " 'leadership',\n",
       " 'league',\n",
       " 'learn',\n",
       " 'learned',\n",
       " 'learning',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'leaves',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'less',\n",
       " 'lesson',\n",
       " 'lessons',\n",
       " 'letting',\n",
       " 'level',\n",
       " 'lies',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'likes',\n",
       " 'link',\n",
       " 'linkedin',\n",
       " 'list',\n",
       " 'listen',\n",
       " 'listening',\n",
       " 'literally',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'liveyourbestlife',\n",
       " 'living',\n",
       " 'lmao',\n",
       " 'lonely',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lose',\n",
       " 'losing',\n",
       " 'lost',\n",
       " 'loud',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'loves',\n",
       " 'loving',\n",
       " 'luck',\n",
       " 'lying',\n",
       " 'made',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'many',\n",
       " 'market',\n",
       " 'mask',\n",
       " 'matter',\n",
       " 'matters',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meant',\n",
       " 'media',\n",
       " 'meditations',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'mentally',\n",
       " 'mentor',\n",
       " 'mess',\n",
       " 'message',\n",
       " 'middle',\n",
       " 'might',\n",
       " 'milk',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'mindmattersmeditations',\n",
       " 'minds',\n",
       " 'mindset',\n",
       " 'mine',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'mirror',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'mistake',\n",
       " 'mistakes',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'mood',\n",
       " 'morning',\n",
       " 'mother',\n",
       " 'move',\n",
       " 'movement',\n",
       " 'movie',\n",
       " 'moving',\n",
       " 'much',\n",
       " 'music',\n",
       " 'must',\n",
       " 'name',\n",
       " 'natural',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'negative',\n",
       " 'netflix',\n",
       " 'never',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nightmares',\n",
       " 'nights',\n",
       " 'nobody',\n",
       " 'normal',\n",
       " 'nothing',\n",
       " 'notice',\n",
       " 'notices',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'nurse',\n",
       " 'nursing',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'officially',\n",
       " 'often',\n",
       " 'okay',\n",
       " 'ones',\n",
       " 'online',\n",
       " 'onto',\n",
       " 'open',\n",
       " 'opportunities',\n",
       " 'opportunity',\n",
       " 'optimism',\n",
       " 'order',\n",
       " 'originally',\n",
       " 'others',\n",
       " 'outside',\n",
       " 'overthink',\n",
       " 'overthinking',\n",
       " 'owntv',\n",
       " 'paid',\n",
       " 'pain',\n",
       " 'painful',\n",
       " 'parents',\n",
       " 'part',\n",
       " 'party',\n",
       " 'passion',\n",
       " 'past',\n",
       " 'path',\n",
       " 'patient',\n",
       " 'patterns',\n",
       " 'peace',\n",
       " 'peanut',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'phone',\n",
       " 'physically',\n",
       " 'pics',\n",
       " 'piece',\n",
       " 'pieces',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'played',\n",
       " 'playing',\n",
       " 'please',\n",
       " 'podcast',\n",
       " 'podcasts',\n",
       " 'point',\n",
       " 'position',\n",
       " 'positive',\n",
       " 'positivethinking',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'power',\n",
       " 'powerful',\n",
       " 'practice',\n",
       " 'pray',\n",
       " 'prayer',\n",
       " 'president',\n",
       " 'pretend',\n",
       " 'pretending',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'principles',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'product',\n",
       " 'promise',\n",
       " 'protect',\n",
       " 'proud',\n",
       " 'public',\n",
       " 'purpose',\n",
       " 'push',\n",
       " 'pushing',\n",
       " 'quarantine',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quiet',\n",
       " 'quit',\n",
       " 'rank',\n",
       " 'rather',\n",
       " 'reach',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'reads',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'realize',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasons',\n",
       " 'receive',\n",
       " 'regardless',\n",
       " 'register',\n",
       " 'regret',\n",
       " 'relationship',\n",
       " 'relationships',\n",
       " 'release',\n",
       " 'remember',\n",
       " 'remind',\n",
       " 'required',\n",
       " 'respect',\n",
       " 'responsibility',\n",
       " 'responsible',\n",
       " 'rest',\n",
       " 'results',\n",
       " 'return',\n",
       " 'right',\n",
       " 'rise',\n",
       " 'risk',\n",
       " 'road',\n",
       " 'room',\n",
       " 'running',\n",
       " 'sacrifice',\n",
       " 'sadness',\n",
       " 'safe',\n",
       " 'said',\n",
       " 'saturday',\n",
       " 'save',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'scared',\n",
       " 'scars',\n",
       " 'school',\n",
       " 'season',\n",
       " 'second',\n",
       " 'seconds',\n",
       " 'secret',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'sell',\n",
       " 'selling',\n",
       " 'send',\n",
       " 'sending',\n",
       " 'sense',\n",
       " 'sent',\n",
       " 'series',\n",
       " 'serve',\n",
       " 'service',\n",
       " 'share',\n",
       " 'shared',\n",
       " 'sharing',\n",
       " 'shit',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'show',\n",
       " 'shows',\n",
       " 'shut',\n",
       " 'sick',\n",
       " 'side',\n",
       " 'sign',\n",
       " 'silence',\n",
       " 'silent',\n",
       " 'simon',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'since',\n",
       " 'single',\n",
       " 'sister',\n",
       " 'sitting',\n",
       " 'situation',\n",
       " 'skills',\n",
       " 'sleep',\n",
       " 'sleeping',\n",
       " 'small',\n",
       " 'smart',\n",
       " 'smile',\n",
       " 'social',\n",
       " 'sold',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'soon',\n",
       " 'soothe',\n",
       " 'soothing',\n",
       " 'sorry',\n",
       " 'soul',\n",
       " 'sound',\n",
       " 'source',\n",
       " 'space',\n",
       " 'speak',\n",
       " 'speaking',\n",
       " 'special',\n",
       " 'spend',\n",
       " 'spent',\n",
       " 'spirit',\n",
       " 'spiritual',\n",
       " 'spiritualcoach',\n",
       " 'spiritualguidance',\n",
       " 'spoke',\n",
       " 'stand',\n",
       " 'standing',\n",
       " 'star',\n",
       " 'stare',\n",
       " 'staring',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'starts',\n",
       " 'startup',\n",
       " 'state',\n",
       " 'stay',\n",
       " 'step',\n",
       " 'steps',\n",
       " 'still',\n",
       " 'stock',\n",
       " 'stop',\n",
       " 'stopped',\n",
       " 'store',\n",
       " 'stories',\n",
       " 'story',\n",
       " 'stream',\n",
       " 'streaming',\n",
       " 'strength',\n",
       " 'stress',\n",
       " 'strong',\n",
       " 'struggle',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'subscribe',\n",
       " 'succeed',\n",
       " 'success',\n",
       " 'sucks',\n",
       " 'suddenly',\n",
       " 'suicidal',\n",
       " 'suicide',\n",
       " 'summer',\n",
       " 'super',\n",
       " 'support',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surprised',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talked',\n",
       " 'talking',\n",
       " 'taught',\n",
       " 'teach',\n",
       " 'team',\n",
       " 'tears',\n",
       " 'tech',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'tells',\n",
       " 'term',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thankful',\n",
       " 'thanks',\n",
       " 'thanksgiving',\n",
       " 'therapist',\n",
       " 'therapy',\n",
       " 'thesamparr',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thinks',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thoughts',\n",
       " 'thread',\n",
       " 'three',\n",
       " 'throw',\n",
       " 'thursday',\n",
       " 'tiktok',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tired',\n",
       " 'title',\n",
       " 'today',\n",
       " 'together',\n",
       " 'told',\n",
       " 'tomorrow',\n",
       " 'tonight',\n",
       " 'took',\n",
       " 'total',\n",
       " 'totally',\n",
       " 'towards',\n",
       " 'treat',\n",
       " 'tried',\n",
       " 'true',\n",
       " 'truly',\n",
       " 'trump',\n",
       " 'trust',\n",
       " 'truth',\n",
       " 'trying',\n",
       " 'tune',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'turns',\n",
       " 'tweet',\n",
       " 'tweets',\n",
       " 'twitch',\n",
       " 'twitter',\n",
       " 'type',\n",
       " 'ugly',\n",
       " 'uncomfortable',\n",
       " 'understand',\n",
       " 'unless',\n",
       " 'update',\n",
       " 'updated',\n",
       " 'upset',\n",
       " 'used',\n",
       " 'useless',\n",
       " 'value',\n",
       " 'vanzant',\n",
       " 'version',\n",
       " 'video',\n",
       " 'viral',\n",
       " 'vision',\n",
       " 'voice',\n",
       " 'vote',\n",
       " 'wait',\n",
       " 'waiting',\n",
       " 'wake',\n",
       " 'walk',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wanting',\n",
       " 'wants',\n",
       " 'warning',\n",
       " 'waste',\n",
       " 'watch',\n",
       " 'watched',\n",
       " 'watching',\n",
       " 'water',\n",
       " 'ways',\n",
       " 'weak',\n",
       " 'wear',\n",
       " 'wearing',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'weird',\n",
       " 'well',\n",
       " 'went',\n",
       " 'whatever',\n",
       " 'whenever',\n",
       " 'whether',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'whose',\n",
       " 'wife',\n",
       " 'willing',\n",
       " 'winning',\n",
       " 'wins',\n",
       " 'wish',\n",
       " 'within',\n",
       " 'without',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'wonder',\n",
       " 'wondering',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'workers',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worry',\n",
       " 'worse',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'worthless',\n",
       " 'would',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'wrong',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yesterday',\n",
       " 'young',\n",
       " 'youtube',\n",
       " 'zoom']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer()\n",
    "X = tf_transformer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVectorizer = TfidfVectorizer(max_features =1000)\n",
    "X = tfidfVectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_s, X_test_s , y_train_s, y_test_s = train_test_split(X, y , test_size = 0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kerillos\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Naive Bayes is a statistical classification technique based on Bayes Theorem\n",
    "# common classifier used in sentiment analysis is the Naive Bayes Classifier.\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier # this is experimental\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifiers = [GradientBoostingClassifier(),GaussianNB(),HistGradientBoostingClassifier(),\n",
    "               RandomForestClassifier(),LogisticRegression(),XGBClassifier(),LGBMClassifier(),\n",
    "               CatBoostClassifier(verbose=0),DecisionTreeClassifier(),KNeighborsClassifier(),SVC()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model GradientBoostingClassifier() \n",
      "--------------\n",
      "Training Accuracy: 0.7678242163491088\n",
      "Testing Accuracy: 0.7252612169637369\n",
      "Testing Confusion Matrix: \n",
      "[[891  53]\n",
      " [394 289]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.94      0.80       944\n",
      "           1       0.85      0.42      0.56       683\n",
      "\n",
      "    accuracy                           0.73      1627\n",
      "   macro avg       0.77      0.68      0.68      1627\n",
      "weighted avg       0.76      0.73      0.70      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model GaussianNB() \n",
      "--------------\n",
      "Training Accuracy: 0.7460049170251998\n",
      "Testing Accuracy: 0.7320221266133989\n",
      "Testing Confusion Matrix: \n",
      "[[537 407]\n",
      " [ 29 654]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.57      0.71       944\n",
      "           1       0.62      0.96      0.75       683\n",
      "\n",
      "    accuracy                           0.73      1627\n",
      "   macro avg       0.78      0.76      0.73      1627\n",
      "weighted avg       0.81      0.73      0.73      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model HistGradientBoostingClassifier() \n",
      "--------------\n",
      "Training Accuracy: 0.8438844499078058\n",
      "Testing Accuracy: 0.7725875845113707\n",
      "Testing Confusion Matrix: \n",
      "[[819 125]\n",
      " [245 438]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82       944\n",
      "           1       0.78      0.64      0.70       683\n",
      "\n",
      "    accuracy                           0.77      1627\n",
      "   macro avg       0.77      0.75      0.76      1627\n",
      "weighted avg       0.77      0.77      0.77      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model RandomForestClassifier() \n",
      "--------------\n",
      "Training Accuracy: 0.9694222495390289\n",
      "Testing Accuracy: 0.7885679164105716\n",
      "Testing Confusion Matrix: \n",
      "[[784 160]\n",
      " [184 499]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       944\n",
      "           1       0.76      0.73      0.74       683\n",
      "\n",
      "    accuracy                           0.79      1627\n",
      "   macro avg       0.78      0.78      0.78      1627\n",
      "weighted avg       0.79      0.79      0.79      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model LogisticRegression() \n",
      "--------------\n",
      "Training Accuracy: 0.8578672403196066\n",
      "Testing Accuracy: 0.8174554394591272\n",
      "Testing Confusion Matrix: \n",
      "[[847  97]\n",
      " [200 483]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85       944\n",
      "           1       0.83      0.71      0.76       683\n",
      "\n",
      "    accuracy                           0.82      1627\n",
      "   macro avg       0.82      0.80      0.81      1627\n",
      "weighted avg       0.82      0.82      0.81      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...) \n",
      "--------------\n",
      "Training Accuracy: 0.8517209588199139\n",
      "Testing Accuracy: 0.7732022126613399\n",
      "Testing Confusion Matrix: \n",
      "[[837 107]\n",
      " [262 421]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82       944\n",
      "           1       0.80      0.62      0.70       683\n",
      "\n",
      "    accuracy                           0.77      1627\n",
      "   macro avg       0.78      0.75      0.76      1627\n",
      "weighted avg       0.78      0.77      0.77      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model LGBMClassifier() \n",
      "--------------\n",
      "Training Accuracy: 0.8452673632452367\n",
      "Testing Accuracy: 0.7732022126613399\n",
      "Testing Confusion Matrix: \n",
      "[[817 127]\n",
      " [242 441]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82       944\n",
      "           1       0.78      0.65      0.71       683\n",
      "\n",
      "    accuracy                           0.77      1627\n",
      "   macro avg       0.77      0.76      0.76      1627\n",
      "weighted avg       0.77      0.77      0.77      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model <catboost.core.CatBoostClassifier object at 0x0000020A71D72A30> \n",
      "--------------\n",
      "Training Accuracy: 0.869698832206515\n",
      "Testing Accuracy: 0.7879532882606023\n",
      "Testing Confusion Matrix: \n",
      "[[842 102]\n",
      " [243 440]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       944\n",
      "           1       0.81      0.64      0.72       683\n",
      "\n",
      "    accuracy                           0.79      1627\n",
      "   macro avg       0.79      0.77      0.77      1627\n",
      "weighted avg       0.79      0.79      0.78      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model DecisionTreeClassifier() \n",
      "--------------\n",
      "Training Accuracy: 0.9694222495390289\n",
      "Testing Accuracy: 0.7627535341118623\n",
      "Testing Confusion Matrix: \n",
      "[[758 186]\n",
      " [200 483]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80       944\n",
      "           1       0.72      0.71      0.71       683\n",
      "\n",
      "    accuracy                           0.76      1627\n",
      "   macro avg       0.76      0.76      0.76      1627\n",
      "weighted avg       0.76      0.76      0.76      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model KNeighborsClassifier() \n",
      "--------------\n",
      "Training Accuracy: 0.8053165334972342\n",
      "Testing Accuracy: 0.6791641057160418\n",
      "Testing Confusion Matrix: \n",
      "[[583 361]\n",
      " [161 522]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.62      0.69       944\n",
      "           1       0.59      0.76      0.67       683\n",
      "\n",
      "    accuracy                           0.68      1627\n",
      "   macro avg       0.69      0.69      0.68      1627\n",
      "weighted avg       0.70      0.68      0.68      1627\n",
      "\n",
      "------------------------------\n",
      "Training Model SVC() \n",
      "--------------\n",
      "Training Accuracy: 0.9383835279655808\n",
      "Testing Accuracy: 0.8266748617086662\n",
      "Testing Confusion Matrix: \n",
      "[[841 103]\n",
      " [179 504]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.86       944\n",
      "           1       0.83      0.74      0.78       683\n",
      "\n",
      "    accuracy                           0.83      1627\n",
      "   macro avg       0.83      0.81      0.82      1627\n",
      "weighted avg       0.83      0.83      0.82      1627\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "for classifier in classifiers:\n",
    "    print(f'Training Model {classifier} \\n--------------')\n",
    "    classifier.fit(X_train_s,y_train_s)\n",
    "    y_pred = classifier.predict(X_test_s)\n",
    "    print(f'Training Accuracy: {classifier.score(X_train_s, y_train_s)}')\n",
    "    print(f'Testing Accuracy: {accuracy_score(y_test_s, y_pred)}')\n",
    "    print(f'Testing Confusion Matrix: \\n{confusion_matrix(y_test_s, y_pred)}')\n",
    "    print(classification_report(y_test_s, y_pred))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9383835279655808\n",
      "Testing Accuracy: 0.8100799016594961\n",
      "Testing Confusion Matrix: \n",
      "[[766 178]\n",
      " [131 552]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       944\n",
      "           1       0.76      0.81      0.78       683\n",
      "\n",
      "    accuracy                           0.81      1627\n",
      "   macro avg       0.81      0.81      0.81      1627\n",
      "weighted avg       0.81      0.81      0.81      1627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_mlp= MLPClassifier(random_state=1, max_iter=500, alpha=0.005)\n",
    "model_mlp.fit(X_train_s,y_train_s)\n",
    "y_pred = model_mlp.predict(X_test_s)\n",
    "print(f'Training Accuracy: {classifier.score(X_train_s, y_train_s)}')\n",
    "print(f'Testing Accuracy: {accuracy_score(y_test_s, y_pred)}')\n",
    "print(f'Testing Confusion Matrix: \\n{confusion_matrix(y_test_s, y_pred)}')\n",
    "print(classification_report(y_test_s, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
